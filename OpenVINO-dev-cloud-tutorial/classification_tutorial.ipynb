{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bd1b5eb",
   "metadata": {},
   "source": [
    "\n",
    "# Classification Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f66d69",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "#### Intel® distribution of OpenVINO™ toolkit overview and terminology\n",
    "\n",
    "Let us begin with a brief overview of the Intel® Distribution of OpenVINO™ toolkit and what this tutorial will be covering.  The Intel® Distribution of OpenVINO™ toolkit enables the quick deployment of convolutional neural networks (CNN) for heterogeneous execution on Intel® hardware while maximizing performance. This is done using the Intel® Deep Learning Deployment Toolkit (Intel® DLDT) included within the Intel® Distribution of OpenVINO™ toolkit with its main components shown below.\n",
    "\n",
    "![image alt text](doc/doc_openvino_overview_image.png)\n",
    "\n",
    "The basic flow is:\n",
    "\n",
    "1. Use a tool, such as Caffe*, to create and train a CNN inference model\n",
    "\n",
    "2. Run the created model through Model Optimizer to produce an optimized Intermediate Representation (IR) stored in files (`.bin` and `.xml`) for use with the Inference Engine\n",
    "\n",
    "3. The User Application then loads and runs models on devices using the Inference Engine and the IR files  \n",
    "\n",
    "This tutorial will focus on the last step, the User Application and using the Inference Engine to run a model on a CPU.\n",
    "\n",
    "##### Using the inference engine\n",
    "\n",
    "Below is a more detailed view of the User Application and Inference Engine:\n",
    "\n",
    "![image alt text](doc/doc_inference_engine_image.png)\n",
    "\n",
    "The Inference Engine includes a plugin library for each supported device that has been optimized for the Intel® hardware device CPU, GPU, and VPU.  From here, we will use the terms \"device\" and “plugin” with the assumption that one infers the other (e.g. CPU device infers the CPU plugin and vice versa).  As part of loading the model, the User Application tells the Inference Engine which device to target which in turn loads the associated plugin library to later run on the associated device. The Inference Engine uses “blobs” for all data exchanges, basically arrays in memory arranged according the input and output data of the model.\n",
    "\n",
    "###### Inference engine API integration flow\n",
    "\n",
    "Using the inference engine API follows the basic steps outlined briefly below.  The API objects and functions will be seen later in the sample code.\n",
    "\n",
    "1. Load the plugin\n",
    "\n",
    "2. Read the model IR\n",
    "\n",
    "3. Load the model into the plugin\n",
    "\n",
    "6. Prepare the input\n",
    "\n",
    "7. Run inference\n",
    "\n",
    "8. Process the output\n",
    "\n",
    "More details on the Inference Engine can be found in the [Inference Engine Development Guide](https://docs.openvinotoolkit.org/latest/_docs_IE_DG_Deep_Learning_Inference_Engine_DevGuide.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4069640a",
   "metadata": {},
   "source": [
    "\n",
    "# Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f554956b",
   "metadata": {},
   "source": [
    "## Create the IR files (Pretrained Model) for the inference model\n",
    "The Intel® Distribution of OpenVINO™ toolkit includes the [Model Optimizer](http://docs.openvinotoolkit.org/latest/_docs_MO_DG_Deep_Learning_Model_Optimizer_DevGuide.html) used to convert and optimize trained models into the Intermediate Representation (IR) model files, and the [Inference Engine](http://docs.openvinotoolkit.org/latest/_docs_IE_DG_Deep_Learning_Inference_Engine_DevGuide.html) that uses the IR model files to run inference on hardware devices.  The IR model files can be created from trained models from popular frameworks (e.g. Caffe\\*, Tensorflow*, etc.). \n",
    "The Intel® Distribution of OpenVINO™ toolkit also includes the [Model Downloader](http://docs.openvinotoolkit.org/latest/_tools_downloader_README.html) utility  to download some common inference models from the [Open Model Zoo](https://github.com/opencv/open_model_zoo). \n",
    "\n",
    "Run the following cell to run the Model Downloader utility with the `--print_all` argument to see all the available inference models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "095fd590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################|| Downloading squeezenet1.1 ||################\n",
      "\n",
      "========== Downloading raw_model/public/squeezenet1.1/squeezenet1.1.prototxt\n",
      "... 100%, 9 KB, 24514 KB/s, 0 seconds passed\n",
      "\n",
      "========== Downloading raw_model/public/squeezenet1.1/squeezenet1.1.caffemodel\n",
      "... 100%, 4834 KB, 23805 KB/s, 0 seconds passed\n",
      "\n",
      "========== Replacing text in raw_model/public/squeezenet1.1/squeezenet1.1.prototxt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!downloader.py --name squeezenet1.1 -o raw_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa345b2",
   "metadata": {},
   "source": [
    "Now, use converter.py to converts the models that are not in the Inference Engine IR format into that format using Model Optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44834388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Converting squeezenet1.1 to IR (FP16)\n",
      "Conversion command: /usr/bin/python3 -m mo --framework=caffe --data_type=FP16 --output_dir=model/public/squeezenet1.1/FP16 --model_name=squeezenet1.1 '--input_shape=[1,3,227,227]' --input=data '--mean_values=data[104.0,117.0,123.0]' --output=prob --input_model=raw_model/public/squeezenet1.1/squeezenet1.1.caffemodel --input_proto=raw_model/public/squeezenet1.1/squeezenet1.1.prototxt\n",
      "\n",
      "Model Optimizer arguments:\n",
      "Common parameters:\n",
      "\t- Path to the Input Model: \t/home/u75443/OpenVINO-dev-cloud-tutorial/raw_model/public/squeezenet1.1/squeezenet1.1.caffemodel\n",
      "\t- Path for generated IR: \t/home/u75443/OpenVINO-dev-cloud-tutorial/model/public/squeezenet1.1/FP16\n",
      "\t- IR output name: \tsqueezenet1.1\n",
      "\t- Log level: \tERROR\n",
      "\t- Batch: \tNot specified, inherited from the model\n",
      "\t- Input layers: \tdata\n",
      "\t- Output layers: \tprob\n",
      "\t- Input shapes: \t[1,3,227,227]\n",
      "\t- Mean values: \tdata[104.0,117.0,123.0]\n",
      "\t- Scale values: \tNot specified\n",
      "\t- Scale factor: \tNot specified\n",
      "\t- Precision of IR: \tFP16\n",
      "\t- Enable fusing: \tTrue\n",
      "\t- Enable grouped convolutions fusing: \tTrue\n",
      "\t- Move mean values to preprocess section: \tNone\n",
      "\t- Reverse input channels: \tFalse\n",
      "Caffe specific parameters:\n",
      "\t- Path to Python Caffe* parser generated from caffe.proto: \t/opt/intel/openvino/deployment_tools/model_optimizer/mo/utils/../front/caffe/proto\n",
      "\t- Enable resnet optimization: \tTrue\n",
      "\t- Path to the Input prototxt: \t/home/u75443/OpenVINO-dev-cloud-tutorial/raw_model/public/squeezenet1.1/squeezenet1.1.prototxt\n",
      "\t- Path to CustomLayersMapping.xml: \t/opt/intel/openvino/deployment_tools/model_optimizer/mo/utils/../../extensions/front/caffe/CustomLayersMapping.xml\n",
      "\t- Path to a mean file: \tNot specified\n",
      "\t- Offsets for a mean file: \tNot specified\n",
      "\t- Inference Engine found in: \t/opt/intel/openvino/python/python3.6/openvino\n",
      "Inference Engine version: \t2021.4.0-3839-cd81789d294-releases/2021/4\n",
      "Model Optimizer version: \t2021.4.0-3839-cd81789d294-releases/2021/4\n",
      "[ SUCCESS ] Generated IR version 10 model.\n",
      "[ SUCCESS ] XML file: /home/u75443/OpenVINO-dev-cloud-tutorial/model/public/squeezenet1.1/FP16/squeezenet1.1.xml\n",
      "[ SUCCESS ] BIN file: /home/u75443/OpenVINO-dev-cloud-tutorial/model/public/squeezenet1.1/FP16/squeezenet1.1.bin\n",
      "[ SUCCESS ] Total execution time: 5.84 seconds. \n",
      "[ SUCCESS ] Memory consumed: 110 MB. \n",
      "\n",
      "========== Converting squeezenet1.1 to IR (FP32)\n",
      "Conversion command: /usr/bin/python3 -m mo --framework=caffe --data_type=FP32 --output_dir=model/public/squeezenet1.1/FP32 --model_name=squeezenet1.1 '--input_shape=[1,3,227,227]' --input=data '--mean_values=data[104.0,117.0,123.0]' --output=prob --input_model=raw_model/public/squeezenet1.1/squeezenet1.1.caffemodel --input_proto=raw_model/public/squeezenet1.1/squeezenet1.1.prototxt\n",
      "\n",
      "Model Optimizer arguments:\n",
      "Common parameters:\n",
      "\t- Path to the Input Model: \t/home/u75443/OpenVINO-dev-cloud-tutorial/raw_model/public/squeezenet1.1/squeezenet1.1.caffemodel\n",
      "\t- Path for generated IR: \t/home/u75443/OpenVINO-dev-cloud-tutorial/model/public/squeezenet1.1/FP32\n",
      "\t- IR output name: \tsqueezenet1.1\n",
      "\t- Log level: \tERROR\n",
      "\t- Batch: \tNot specified, inherited from the model\n",
      "\t- Input layers: \tdata\n",
      "\t- Output layers: \tprob\n",
      "\t- Input shapes: \t[1,3,227,227]\n",
      "\t- Mean values: \tdata[104.0,117.0,123.0]\n",
      "\t- Scale values: \tNot specified\n",
      "\t- Scale factor: \tNot specified\n",
      "\t- Precision of IR: \tFP32\n",
      "\t- Enable fusing: \tTrue\n",
      "\t- Enable grouped convolutions fusing: \tTrue\n",
      "\t- Move mean values to preprocess section: \tNone\n",
      "\t- Reverse input channels: \tFalse\n",
      "Caffe specific parameters:\n",
      "\t- Path to Python Caffe* parser generated from caffe.proto: \t/opt/intel/openvino/deployment_tools/model_optimizer/mo/utils/../front/caffe/proto\n",
      "\t- Enable resnet optimization: \tTrue\n",
      "\t- Path to the Input prototxt: \t/home/u75443/OpenVINO-dev-cloud-tutorial/raw_model/public/squeezenet1.1/squeezenet1.1.prototxt\n",
      "\t- Path to CustomLayersMapping.xml: \t/opt/intel/openvino/deployment_tools/model_optimizer/mo/utils/../../extensions/front/caffe/CustomLayersMapping.xml\n",
      "\t- Path to a mean file: \tNot specified\n",
      "\t- Offsets for a mean file: \tNot specified\n",
      "\t- Inference Engine found in: \t/opt/intel/openvino/python/python3.6/openvino\n",
      "Inference Engine version: \t2021.4.0-3839-cd81789d294-releases/2021/4\n",
      "Model Optimizer version: \t2021.4.0-3839-cd81789d294-releases/2021/4\n",
      "[ SUCCESS ] Generated IR version 10 model.\n",
      "[ SUCCESS ] XML file: /home/u75443/OpenVINO-dev-cloud-tutorial/model/public/squeezenet1.1/FP32/squeezenet1.1.xml\n",
      "[ SUCCESS ] BIN file: /home/u75443/OpenVINO-dev-cloud-tutorial/model/public/squeezenet1.1/FP32/squeezenet1.1.bin\n",
      "[ SUCCESS ] Total execution time: 4.86 seconds. \n",
      "[ SUCCESS ] Memory consumed: 106 MB. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "!converter.py --name squeezenet1.1 -d raw_model -o model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a21c45",
   "metadata": {},
   "source": [
    "# Tutorial Application"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df743fd7",
   "metadata": {},
   "source": [
    "## Imports\n",
    "Module:\n",
    "- [os](https://docs.python.org/3/library/os.html#module-os) - Operating system specific module (used for file name parsing)\n",
    "- [cv2](https://docs.opencv.org/trunk/) - OpenCV module\n",
    "- [time](https://docs.python.org/3/library/time.html#module-time) - time tracking module (used for measuring execution time)\n",
    "- [numpy](http://www.numpy.org/) - n-dimensional array manipulation, e.g:import numpy as np\n",
    "- [openvino.inference_engine](https://software.intel.com/en-us/articles/OpenVINO-InferEngine) - the IENetwork and IECore objects e.g: from openvino.inference_engine import IECore\n",
    "- [matplotlib](https://matplotlib.org/) - pyplot is used for displaying output images e.g: from matplotlib import pyplot as plt\n",
    "\n",
    "<br><div class=tip><b>Example: </b>\n",
    "- import os\n",
    "\n",
    "Run the cell below to import Python dependencies needed for displaying the results in this notebook. \n",
    "\n",
    "<br><div class=tip><b>Tip: </b>Select a cell and then use **Ctrl+Enter** to run that cell.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2585fb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the module in this cell\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e19ac48",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "Here we will create and set the following configuration parameters\n",
    "We will set all parameters here only once except for *input_path* which we will change later to point to different images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35b402b0",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-3-da4bede69347>, line 20)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-3-da4bede69347>\"\u001b[0;36m, line \u001b[0;32m20\u001b[0m\n\u001b[0;31m    report_top_n = ?\u001b[0m\n\u001b[0m                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# model IR files\n",
    "# model path: model/public/squeezenet1.1/FP32/ for squeezenet1.1.xml, squeezenet1.1.bin\n",
    "# Code here\n",
    "model_xml = \".../squeezenet1.1.xml\"\n",
    "model_bin = \".../squeezenet1.1.bin\"\n",
    "\n",
    "# input image file from path image, here we use dog.jpg\n",
    "# e.g: images/monkey.jpg\n",
    "# Code here\n",
    "input_path = \"\"\n",
    "   \n",
    "# CPU extension library to use\n",
    "cpu_extension_path = os.path.expanduser(\"~\")+\"/inference_engine_samples/intel64/Release/lib/libcpu_extension.so\"\n",
    "\n",
    "# device to use\n",
    "device = \"CPU\"\n",
    "\n",
    "# Number of top-n classification results to report, we use 10\n",
    "# Code here\n",
    "report_top_n = ?\n",
    "\n",
    "# output labels - \"squeezenet1.1.labels\"\n",
    "#Path to labels mapping file used to map outputted integers to strings (e.g. 7=\"car\")\n",
    "# Code here\n",
    "labels_path = \"squeezenet1.1.labels\"\n",
    "\n",
    "#print out the value for debug purpose\n",
    "# Code here \n",
    "print(\"Configuration parameters settings:\"\n",
    "     \"\\n\\tmodel_xml=\", ?,\n",
    "      \"\\n\\tmodel_bin=\", ?,\n",
    "      \"\\n\\tinput_path=\", ?,\n",
    "      \"\\n\\tdevice=\", ?, \n",
    "      \"\\n\\tlabels_path=\", ?, \n",
    "      \"\\n\\treport_top_n=\", ?)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef11f237",
   "metadata": {},
   "source": [
    "<b>Expected Output:</b>\n",
    "\n",
    "<p> Configuration parameters settings:</p>\n",
    "<p>\tmodel_xml= model/public/squeezenet1.1/FP32/squeezenet1.1.xml </p>\n",
    "<p>\tmodel_bin= model/public/squeezenet1.1/FP32/squeezenet1.1.bin </p>\n",
    "<p>\tinput_path= images/monkey.jpg </p>\n",
    "<p>\tdevice= CPU </p>\n",
    "<p>\tlabels_path= squeezenet1.1.labels </p>\n",
    "<p>\treport_top_n= 10 </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe39f919",
   "metadata": {},
   "source": [
    "## Create network\n",
    "\n",
    "Here we create an IENetwork object and load the model's IR files into it. After loading the model, we check to make sure that all the model's layers are supported by the plugin we will use. We also check to make sure that the model's input and output are as expected for later when we run inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1adedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Inference Engine instance\n",
    "ie = IECore()\n",
    "print(\"An Inference Engine object has been created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6df5b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load network from IR files\n",
    "# ie.read_network(model=xml xml, weights=bin file)\n",
    "# Code here\n",
    "net = ie.read_network(model=?, weights=?)\n",
    "print(\"Loaded model IR files [\",model_bin,\"] and [\", model_xml, \"]\\n\")\n",
    "\n",
    "# check to make sure that the plugin has support for all layers in the loaded model\n",
    "supported_layers = ie.query_network(net,device)\n",
    "\n",
    "\n",
    "# check to make sue that the model's input and output are what is expected\n",
    "assert len(net.input_info.keys()) == 1, \\\n",
    "    \"ERROR: This sample supports only single input topologies\"\n",
    "assert len(net.outputs) == 1, \\\n",
    "    \"ERROR: This sample supports only single output topologies\"\n",
    "print(\"SUCCESS: Model IR files have been loaded and verified\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ca4679",
   "metadata": {},
   "source": [
    "<b>Expected Output:</b>\n",
    "\n",
    "Loaded model IR files [ model/public/squeezenet1.1/FP32/squeezenet1.1.bin ] and [ model/public/squeezenet1.1/FP32/squeezenet1.1.xml ]\n",
    "\n",
    "SUCCESS: Model IR files have been loaded and verified"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0bed034",
   "metadata": {},
   "source": [
    "## Load model\n",
    "\n",
    "Here we load the model network into the plugin so that we may run inference.  `exec_net` will be used later to actually run inference.  After loading, we store the names of the input (`input_blob`) and output (`output_blob`) blobs to use when accessing the input and output blobs of the model.  Lastly, we store the model's input dimensions into the following variables:\n",
    "- `n` = input batch size\n",
    "- `c` = number of input channels (here 1 channel per color R,G, and B)\n",
    "- `h` = input height\n",
    "- `w` = input width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10be0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "exec_net = ie.load_network(network=net, num_requests=2, device_name=device)\n",
    "\n",
    "# store name of input and output blobs\n",
    "input_blob = next(iter(net.input_info))\n",
    "output_blob = next(iter(net.outputs))\n",
    "\n",
    "# read the input's dimensions: n=batch size, c=number of channels, h=height, w=width\n",
    "# Assign the dimension variables as the object of \"net\" class\n",
    "# Code here\n",
    "?,?,?,? = net.input_info[input_blob].input_data.shape\n",
    "\n",
    "print(\"Loaded model into Inference Engine for device:\", device, \n",
    "      \"\\nModel input dimensions: n=\",n,\", c=\",c,\", h=\",h,\", w=\",w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e4e848",
   "metadata": {},
   "source": [
    "<b>Expected Output:</b>\n",
    "\n",
    "Loaded model into Inference Engine for device: CPU \n",
    "Model input dimensions: n= 1 , c= 3 , h= 227 , w= 227"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df9eae1",
   "metadata": {},
   "source": [
    "## Load labels\n",
    "\n",
    "Here, if the `labels_path` variable has been set to point to a label mapping file, we open the file and load the labels into the variable `labels_map`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a30cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_map = None\n",
    "# if labels points to a label mapping file, then load the file into labels_map\n",
    "print(labels_path)\n",
    "# os.path.isfile -> check if the input is a file \n",
    "if os.path.isfile(labels_path):\n",
    "    with open(labels_path, 'r') as f:\n",
    "        # load the mapping data from labels_path to \"labels_map\"\n",
    "        # Code here\n",
    "        ? = [x.split(sep=' ', maxsplit=1)[-1].strip() for x in f]\n",
    "    print(\"Loaded label mapping file [\",labels_path,\"]\")\n",
    "else:\n",
    "    print(\"No label mapping file has been loaded, only numbers will be used\",\n",
    "          \" for detected object labels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446875ea",
   "metadata": {},
   "source": [
    "Expected Output:\n",
    "\n",
    "Loaded label mapping file [ squeezenet1.1.labels ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0253055",
   "metadata": {},
   "source": [
    "## Prepare input\n",
    " We define the functions `loadInputImage()` and `resizeInputImage()` for the operations so that we may reuse them again later in the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3afa06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to load an input image\n",
    "def loadInputImage(input_path):\n",
    "    # globals to store input width and height\n",
    "    global input_w, input_h\n",
    "    \n",
    "    # use OpenCV to load the input image, e.g: cv2.VideoCapture(<string path>)\n",
    "    # here code \n",
    "    cap = ? \n",
    "    \n",
    "    # store input width and height\n",
    "    input_w = cap.get(3)\n",
    "    input_h = cap.get(4)\n",
    "    print(\"Loaded input image [\",input_path,\"], resolution=\", input_w, \"w x \",\n",
    "          input_h, \"h\")\n",
    "\n",
    "    # load the input image\n",
    "    ret, image = cap.read()\n",
    "    del cap\n",
    "    return image\n",
    "\n",
    "# define function for resizing input image\n",
    "def resizeInputImage(image):\n",
    "    # resize image dimensions form image to model's input w x h\n",
    "    in_frame = cv2.resize(image, (w, h))\n",
    "    # Change data layout from HWC to CHW\n",
    "    in_frame = in_frame.transpose((2, 0, 1))  \n",
    "    # reshape to input dimensions\n",
    "    in_frame = in_frame.reshape((n, c, h, w))\n",
    "    print(\"Resized input image from {} to {}\".format(image.shape[:-1], (h, w)))\n",
    "    return in_frame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc31150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load image\n",
    "# load the input_path we defined early, hint: check the name of \n",
    "# variable in configuration cell\n",
    "# Code here\n",
    "image = loadInputImage(?)\n",
    "\n",
    "# resize the input image\n",
    "in_frame = resizeInputImage(?)\n",
    "\n",
    "# display input image\n",
    "print(\"Input image:\")\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ce59dc",
   "metadata": {},
   "source": [
    "<b>Expected Output:</b>\n",
    "\n",
    "Loaded input image [ ./dog.jpg ], resolution= 3124.0 w x  4020.0 h\n",
    "Resized input image from (4020, 3124) to (227, 227)\n",
    "<p>Input image:</p>\n",
    "<img src=\"doc/dog.jpg\" alt=\"Drawing\" style=\"width: 150px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f746e09d",
   "metadata": {},
   "source": [
    "# Run inference\n",
    "\n",
    "Now that we have the input image in the correct format for the model, we now run inference on the input image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75abc404",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save start time \n",
    "# Question: Why do we need to calculate the time?\n",
    "inf_start = time.time()\n",
    "\n",
    "# run inference with the \"resized image\", do you still remember it?\n",
    "# Code here\n",
    "res = exec_net.infer(inputs={input_blob: ?})   \n",
    "\n",
    "# Question: What is \"res\" behind?\n",
    "\n",
    "# calculate time from start until now\n",
    "inf_time = time.time() - inf_start\n",
    "print(\"Inference complete, run time: {:.3f} ms\".format(inf_time * 1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b79a58",
   "metadata": {},
   "source": [
    "## Process and display results\n",
    "\n",
    "Process the top `report_top_n` items.  We define the function `processAndDisplayResults()` so that we may use it again later in the tutorial to process results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9c2582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create function to process inference results\n",
    "def processAndDisplayResults(probs, orig_input_image, orig_input_path):\n",
    "    # display image\n",
    "    plt.figure()\n",
    "    plt.axis(\"off\")\n",
    "    im_to_show = cv2.cvtColor(orig_input_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # report top n results for image\n",
    "    print(\"Top \", report_top_n, \" results for image\",orig_input_path,\":\")\n",
    "    \n",
    "    # remove dimensions of length=1\n",
    "    probs = np.squeeze(probs)\n",
    "    \n",
    "    # sort then return top report_top_n entries\n",
    "    top_ind = np.argsort(probs)[-report_top_n:][::-1]\n",
    "    \n",
    "    # show input image\n",
    "    plt.imshow(im_to_show)\n",
    "    \n",
    "    # print out top probabilities, looking up label\n",
    "    print(\"Probability% is <label>\")\n",
    "    for id in top_ind:\n",
    "        det_label = labels_map[id] if labels_map else \"#{}\".format(id)\n",
    "        print(\" {:.7f} % is {}\".format(probs[id], det_label))\n",
    "    print(\"\\n\")\n",
    "\n",
    "processAndDisplayResults(res[output_blob][0], image, input_path)\n",
    "print(\"Processed and displayed inference output results.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bed2079",
   "metadata": {},
   "source": [
    "<b>Expected Output:</b>\n",
    "\n",
    "<p> Top  10  results for image ./dog.jpg :</p> \n",
    "<p>Probability% is <label></p> \n",
    "<p>0.2329701 % is Chihuahua</p> \n",
    "<p>0.0853880 % is husky</p> \n",
    "<p>0.0783308 % is dog, husky</p> \n",
    "<p>0.0472347 % is pug-dog</p> \n",
    " <p>  ...</p> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6824bcf",
   "metadata": {},
   "source": [
    "## Exercise #1: Run your own image\n",
    "\n",
    "Now that we have seen all the steps, let us run them again on a different image.  We also define `inferImage()` to combine the input processing, inference, and processing and displaying results so that we may use it again later in the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea94fa52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to prepare input, run inference, and process inference results\n",
    "\n",
    "# Code here\n",
    "\n",
    "# prepare input, load?resized? revise the flow from cells above\n",
    "\n",
    "\n",
    "# run inference\n",
    "\n",
    "\n",
    "# process inference results \n",
    "\n",
    "\n",
    "# set path to different input image\n",
    "input_path = ?\n",
    "\n",
    "# load input image\n",
    "image = ?\n",
    "\n",
    "# infer image and display results\n",
    "inferImage(image, input_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (OpenVINO 2021.4 LTS)",
   "language": "python",
   "name": "c003-python_3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
